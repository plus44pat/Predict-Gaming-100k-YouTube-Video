{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMnGHfBRtSkzdZ/0gGDMisQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VfUvJyXNIWFt"},"outputs":[],"source":["'''\n","Part 1\n","Goal: Find gaming videos.\n","Description: I use a multitude of search queries to find a lot of videos. I had to do a multitude of search queries because I am limited to 550 videos per search. Here are all the search parameters I looped through\n","Additional Queries: These are words that are popular through Google Trends.\n","Video Durations: I used Short, Medium, and Long to find a variety of length of videos\n","Published After/Published Before: This was used to find a multitude of videos per day during the time period of 1/29 to ⅖.\n","These are the parameters of the search that never change\n","videoCategoryId=20: This never changed and this is the Category for Gaming Videos\n","regionCode=US: This is for videos found in the US\n","relevanceLanguage=en: This is for English Videos\n","maxResults=50: This is the most videos you can find per page\n","Output: Here is the csv it leads to (https://drive.google.com/file/d/1hCP06fC4PFFZjzsy1_MNrhHc4s3-wLaK/view)\n","\n","'''\n","\n","import requests\n","import pandas as pd\n","from datetime import datetime, timedelta\n","import os\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# URL template for subsequent requests. This uses the Youtube API\n","url_template = 'https://yt4.lemnoslife.com/noKey/search?part=snippet&type=video&videoDuration={}&videoCategoryId=20&publishedAfter={}&publishedBefore={}&order={}&regionCode=US&relevanceLanguage=en&maxResults=50&pageToken={}&q={}'\n","# Category id 20 is built into the URL to get gaming videos. My goal is to see what characteristics a video needs to be a popular video\n","# Define the list of additional search queries. I used these words to find games. I also used no query as well to find videos\n","additional_queries = ['game', 'sonic', 'gameplay', 'games', 'gta', 'mario', 'kids', 'tekken', 'pokemon', 'tekken 8', 'play', 'roblox', 'fortnite', 'music', 'warzone', 'fnf', 'type beat', 'xbox', 'gta 5', 'minecraft', 'madden', 'call of duty', 'freestyle', 'slime', 'guide']\n","\n","# Define the list of orders. This allows me to search for games in different ways. I need this because I am only limited to 550 videos and 11 page tokens per search\n","orders = ['date', 'rating', 'relevance', 'title', 'videoCount', 'viewCount']\n","\n","# Define the list of video durations. This allows to me to get various types of videos\n","video_durations = ['short', 'medium', 'long']\n","\n","def fetch_data(url):\n","    response = requests.get(url)\n","    return response.json()\n","\n","def process_response(published_after, published_before, query, order, video_duration):\n","    next_page_token = ''\n","    all_videos_data = []\n","    while True:\n","        url = url_template.format(video_duration, published_after, published_before, order, next_page_token, query)\n","        print(\"URL:\", url)  # Print the URL being used\n","\n","        try:\n","            data = fetch_data(url)\n","            if 'items' in data:\n","                for item in data['items']:\n","                    video_id = item['id']['videoId']\n","                    snippet = item['snippet']\n","                    published_at = snippet.get('publishedAt', '')\n","                    channel_id = snippet.get('channelId', '')\n","                    title = snippet.get('title', '')\n","                    description = snippet.get('description', '')\n","\n","                    all_videos_data.append({\n","                        'Video ID': video_id,\n","                        'Published At': published_at,\n","                        'Channel ID': channel_id,\n","                        'Title': title,\n","                        'Description': description\n","                    })\n","\n","                if 'nextPageToken' in data:\n","                    next_page_token = data['nextPageToken']\n","                else:\n","                    break\n","            else:\n","                print(\"No items found in response. End of results\")\n","                break\n","        except Exception as e:\n","            print(\"Error:\", e)\n","            break\n","    return all_videos_data\n","\n","def main():\n","    start_date = datetime(2024, 1, 29)\n","    end_date = datetime(2024, 2, 5)\n","\n","    for current_date in pd.date_range(start_date, end_date, freq='D'):\n","        published_after = current_date.strftime('%Y-%m-%dT00:00:00Z')\n","        published_before = (current_date + timedelta(days=1)).strftime('%Y-%m-%dT00:00:00Z')\n","        print(\"Published After:\", published_after)\n","        print(\"Published Before:\", published_before)\n","\n","        final_result_df = pd.DataFrame()\n","\n","        for query in additional_queries:\n","            for order in orders:\n","                for video_duration in video_durations:\n","                    videos_data = process_response(published_after, published_before, query, order, video_duration)\n","                    final_result_df = pd.concat([final_result_df, pd.DataFrame(videos_data)])\n","\n","        final_result_df.drop_duplicates(subset=['Video ID'], inplace=True)\n","\n","        csv_filename = r'C:\\Users\\sulli\\Downloads\\Youtube Folder\\search_results.csv'\n","        # Check if the file exists and create it if it doesn't\n","        if not os.path.exists(csv_filename):\n","            final_result_df.to_csv(csv_filename, mode='w', index=False)\n","        else:\n","            final_result_df.to_csv(csv_filename, mode='a', index=False, header=False)\n","        print(f\"Data for {current_date.strftime('%Y-%m-%d')} appended to {csv_filename}\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n","'''\n","    Part 2\n","    Goal: Find information on each video\n","Description: I use this to find information points on Youtube Videos. Here are some examples of what I collected:\n","What Channel the video is from\n","What tags are used on the video\n","What is the description of the video\n","What is the title of the video\n","I am trying to collect features for a video to understand what works and what doesn’t for a high viewing youtube video\n","Output: Here is the csv it leads to (https://drive.google.com/file/d/1kJ3E83pcbLmw8nn2__r3J_JbnLG4LEkL/view)\n","'''\n","\n","import pandas as pd\n","import requests\n","import time\n","\n","\n","# File path in your Google Drive\n","search_results_file_path = '/content/drive/My Drive/Capstone Project 2/search_results.csv'\n","video_info_file_path = '/content/drive/My Drive/Capstone Project 2/search_results_video_info.csv'\n","\n","# Load CSV file containing new video IDs\n","search_results_df = pd.read_csv(search_results_file_path)\n","\n","# Extract video IDs\n","video_ids_all = set(search_results_df['Video ID'])\n","\n","# If there are no new video IDs, exit\n","if not video_ids_all:\n","    print(\"No new video IDs to fetch.\")\n","    exit()\n","\n","# Convert video IDs to a comma-separated string\n","video_id_string = ','.join(video_ids_all)\n","\n","# Determine the batch size\n","batch_size = min(len(video_ids_all), 50)\n","\n","# Calculate the total number of batches\n","num_batches = ((len(video_ids_all) - 1) // batch_size) + 1\n","\n","# Function to fetch data from URL synchronously with retries\n","def fetch_data_with_retries(url, retries=3):\n","    for _ in range(retries):\n","        try:\n","            response = requests.get(url)\n","            if response.status_code == 200:\n","                return response.json()\n","            else:\n","                print(f\"Request failed. Retrying...\")\n","                time.sleep(1)  # Adding a small delay before retrying\n","        except Exception as e:\n","            print(f\"Error occurred: {e}. Retrying...\")\n","            time.sleep(1)  # Adding a small delay before retrying\n","    print(\"Failed after multiple retries. Skipping this batch.\")\n","    return None\n","\n","# Main function to process batches synchronously\n","def process_batches():\n","    for batch_index in range(num_batches):\n","        start_index = batch_index * batch_size\n","        end_index = min((batch_index + 1) * batch_size, len(video_ids_all))\n","\n","        # Extract video IDs for the current batch\n","        video_ids_batch = list(video_ids_all)[start_index:end_index]\n","        video_id_string_batch = ','.join(video_ids_batch)\n","\n","        # Construct URL for batch request\n","        url = f\"https://yt.lemnoslife.com/noKey/videos?part=snippet,contentDetails,statistics,liveStreamingDetails,status&id={video_id_string_batch}\"\n","\n","        # Fetch data from URL synchronously with retries\n","        data = fetch_data_with_retries(url)\n","        if data is not None and 'items' in data:\n","            # Create list to store rows for current batch\n","            batch_rows = []\n","            for item in data['items']:\n","                # Process each item and append to batch_rows\n","                video_id = item['id']\n","                snippet = item.get('snippet', {})\n","                content_details = item.get('contentDetails', {})\n","                statistics = item.get('statistics', {})\n","                live_streaming_details = item.get('liveStreamingDetails', {})\n","                status = item.get('status', {})\n","\n","                channel_id = snippet.get('channelId', '')\n","                channel_title = snippet.get('channelTitle', '')\n","                tags = ','.join(snippet.get('tags', []))\n","                duration = content_details.get('duration', '')\n","                content_rating = content_details.get('contentRating', '')\n","                definition = content_details.get('definition', '')\n","                caption = content_details.get('caption', '')\n","                licensed_content = content_details.get('licensedContent', '')\n","                projection = content_details.get('projection', '')\n","                dimension = content_details.get('dimension', '')\n","\n","                yt_rating = content_details.get('contentRating', {}).get('ytRating', '')\n","                made_for_kids = status.get('madeForKids', '')\n","                self_declared_made_for_kids = status.get('selfDeclaredMadeForKids', '')\n","                view_count = statistics.get('viewCount', '')\n","                like_count = statistics.get('likeCount', '')\n","                stream_start_time = live_streaming_details.get('actualStartTime', '')\n","                stream_end_time = live_streaming_details.get('actualEndTime', '')\n","                published_at = snippet.get('publishedAt', '')\n","                title = snippet.get('title', '')\n","                description = snippet.get('description', '')\n","\n","                # Append row to batch_rows list if Stream Start Time is null\n","                if stream_start_time is None:\n","                    batch_rows.append({\n","                        'Video ID': video_id,\n","                        'Channel ID': channel_id, #This allows me to look up channel information in the future\n","                        'Channel Title': channel_title,\n","                        'Tags': tags, #Tags are used to allow people to find videos easier or even click on tags to search for a particular kind of video\n","                        'Duration': duration, #This will allow me to see if the length of the video matters for popularity\n","                        'Content Rating': content_rating, #This is similar to movie ratings\n","                        'YT Rating': yt_rating, #This is a subset of content rating\n","                        'Made For Kids': made_for_kids, #Youtube has a kids app. This will decide if it is on the kids app or not. If on the kids app, it will get to a wider audience\n","                        'Self Declared Made For Kids': self_declared_made_for_kids, #This generally was empty\n","                        'View Count': view_count, #The main thing I am looking for, views\n","                        'Like Count': like_count, #This is when users click on the thumbs up button in youtube and favorite a video. Supposedly it puts it more into the algorithm and on the trending page\n","                        'Stream Start Time': stream_start_time, #This allows me to find if it streamed or not. Also, creators can use a live function where they post the video early and people can react together about the video\n","                        'Stream End Time': stream_end_time, #This will help me figure out if the Youtube Premier function was used\n","                        'Published At': published_at, # When the video was created in UTC-0\n","                        'Video Title': title, #What was the title of the video\n","                        'Video Description': description, #This is the description used for the video\n","                        'Video Definition': definition, #This will tell you what quality the video is\n","                        'Video Captions': caption, #This tells me if Captions are available\n","                        'Video Licensed': licensed_content, #This will tell me if the Video is licensed https://support.google.com/youtube/answer/2797468?hl=en\n","                        'Video Projection': projection, #This will tell you if it is a normal video or a 360 video\n","                        'Video Dimension': dimension #This will tell you if it is 2d or 3d\n","                    })\n","\n","            # Create DataFrame for current batch\n","            batch_data = pd.DataFrame(batch_rows)\n","\n","            # Save batch DataFrame to CSV in append mode\n","            batch_data.to_csv(video_info_file_path, mode='a', index=False, header=not bool(batch_index))\n","\n","        # Calculate remaining videos after this batch\n","        remaining_videos = len(video_ids_all) - ((batch_index + 1) * batch_size)\n","        remaining_videos = max(remaining_videos, 0)  # Ensure remaining videos count is not negative\n","\n","        # Print remaining videos\n","        print(f\"Batch {batch_index + 1} completed. {remaining_videos} videos remaining.\")\n","\n","# Run the process_batches function\n","start_time = time.time()\n","process_batches()\n","end_time = time.time()\n","print(f\"Total processing time: {end_time - start_time} seconds.\")\n","\n","'''\n","Part 3\n","Goal: Convert duration to seconds and remove Shorts\n","Description: Convert duration into seconds and remove anything that is less than 60 seconds. Anything less than 60 seconds counts as a short and has a different advertising model. Therefore, I am only concerned with videos over 60 seconds.\n","Output: Here is the csv it leads to https://drive.google.com/file/d/1d5qPg3Fu974CQlknLQb_kGrWJKORj2nH/view\n","\n","'''\n","import pandas as pd\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Read the CSV file into a DataFrame\n","search_results_df = pd.read_csv('/content/drive/My Drive/Capstone Project 2/search_results_video_info.csv')\n","\n","#This will convert duration into seconds. I will call this field duration_seconds\n","def extract_seconds(duration_str):\n","    print(\"Processing duration:\", duration_str)\n","\n","    if pd.isna(duration_str) or duration_str == '' or duration_str == 'P0D':  # Check if duration_str is null, empty, or 'P0D'\n","        print(\"Output:\", 0)\n","        return 0\n","\n","    total_seconds = 0\n","\n","    if 'S' in duration_str:  # If seconds are present\n","        index_s = duration_str.find('S')\n","        seconds_str = duration_str[index_s-2:index_s]\n","        if seconds_str.isdigit():  # If the substring before 'S' is a number\n","            total_seconds += int(seconds_str)\n","        else:  # If the substring before 'S' is not a number\n","            total_seconds += int(duration_str[index_s-1])\n","\n","    if 'M' in duration_str:  # If minutes are present\n","        index_m = duration_str.find('M')\n","        minutes_str = duration_str[index_m-2:index_m]\n","        if minutes_str.isdigit():  # If the substring before 'M' is a number\n","            total_seconds += int(minutes_str) * 60\n","        else:  # If the substring before 'M' is not a number\n","            total_seconds += int(duration_str[index_m-1]) * 60\n","\n","    if 'H' in duration_str:  # If hours are present\n","        index_h = duration_str.find('H')\n","        hours_str = duration_str[index_h-2:index_h]\n","        if hours_str.isdigit():  # If the substring before 'H' is a number\n","            total_seconds += int(hours_str) * 3600\n","        else:  # If the substring before 'H' is not a number\n","            total_seconds += int(duration_str[index_h-1]) * 3600\n","\n","    if 'D' in duration_str:  # If days are present\n","        index_d = duration_str.find('D')\n","        days_str = duration_str[index_d-2:index_d]\n","        if days_str.isdigit():  # If the substring before 'D' is a number\n","            total_seconds += int(days_str) * 86400\n","        else:  # If the substring before 'D' is not a number\n","            total_seconds += int(duration_str[index_d-1]) * 86400\n","\n","    print(\"Output:\", total_seconds)\n","    return total_seconds\n","\n","# Apply extract_seconds to the Duration column to convert it into seconds\n","search_results_df['duration_seconds'] = search_results_df['Duration'].apply(extract_seconds)\n","\n","# Remove any videos shorter than 60 seconds\n","search_results_df = search_results_df[search_results_df['duration_seconds'] >= 60]\n","\n","# Output CSV file path. I have been storing things in csvs so I have a database to pull from\n","output_csv_file = '/content/drive/My Drive/Capstone Project 2/search_results_video_info_no_shorts.csv'\n","\n","# Write the final results to the CSV file\n","search_results_df.to_csv(output_csv_file, index=False)\n","\n","print(search_results_df.info())\n","\n","'''\n","Part 4\n","Goal: Find information on the channel for each video\n","Description: Trying to find info on the channel that should help understand a successful Youtube video. Here are some examples of the info being collected:\n","Country Origin of the Channel\n","Date Channel it is created\n","Is the Channel Made for Kids\n","Channel Subscriber Count\n","Channel Views\n","Output: Here is the csv it leads to https://drive.google.com/file/d/10MrSXtNPSAYpOs3BDFdqXVcegw42Lr7_/view\n","'''\n","import pandas as pd\n","import requests\n","import time\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# File paths\n","search_results_file_path = '/content/drive/My Drive/Capstone Project 2/search_results_video_info_no_shorts.csv'\n","video_info_file_path = '/content/drive/My Drive/Capstone Project 2/search_results_video_info_no_shorts_with_channel_info.csv'\n","\n","# Function to fetch data from URL synchronously with retries\n","def fetch_data_with_retries(url, retries=3):\n","    for _ in range(retries):\n","        try:\n","            print(f\"Fetching data from URL: {url}\")  # Print the URL being accessed\n","            response = requests.get(url)\n","            if response.status_code == 200:\n","                return response.json()\n","            else:\n","                print(f\"Request failed. Retrying...\")\n","                time.sleep(1)  # Adding a small delay before retrying\n","        except Exception as e:\n","            print(f\"Error occurred: {e}. Retrying...\")\n","            time.sleep(1)  # Adding a small delay before retrying\n","    print(\"Failed after multiple retries. Skipping this batch.\")\n","    return None\n","\n","# Function to process all batches and append to CSV\n","def main():\n","    # Load CSV file containing new channel IDs\n","    search_results_df = pd.read_csv(search_results_file_path)\n","    # Extract channel IDs\n","    channel_ids_all = set(search_results_df['Channel ID'].unique())\n","    # If there are no new video IDs, exit\n","    if not channel_ids_all:\n","        print(\"No new channel IDs to fetch.\")\n","        return\n","\n","    # Check the CSV file to see which videos still need to be processed\n","    try:\n","        processed_channel_df = pd.read_csv(video_info_file_path)\n","    except FileNotFoundError:\n","        processed_channel_df = pd.DataFrame()\n","\n","    processed_channel_ids = set(processed_channel_df['Channel ID']) if not processed_channel_df.empty else set()\n","    channel_ids_remaining = channel_ids_all - processed_channel_ids\n","\n","    total_channels_remaining = len(channel_ids_remaining)\n","    print(f\"Total channels remaining to be processed: {channel_ids_remaining}\")\n","\n","    # Determine the batch size\n","    batch_size = min(total_channels_remaining, 50)\n","    # Calculate the total number of batches\n","    num_batches = ((total_channels_remaining - 1) // batch_size) + 1\n","\n","    # Fetch data for remaining videos\n","    batch_rows = []\n","    for i in range(num_batches):\n","        start_index = i * batch_size\n","        end_index = min((i + 1) * batch_size, len(channel_ids_remaining))\n","        channel_ids_batch = list(channel_ids_remaining)[start_index:end_index]\n","        channel_id_string_batch = ','.join(channel_ids_batch)\n","        url = f\"https://yt4.lemnoslife.com/noKey/channels?part=snippet,statistics,status&id={channel_id_string_batch}\"\n","        data = fetch_data_with_retries(url)\n","\n","        if data is not None and 'items' in data:\n","            for item in data['items']:\n","                # Process each item and append to batch_rows\n","                channel_id = item['id']\n","                subscriber_count = item['statistics'].get('subscriberCount', None) #Allows me to see sub count\n","                channel_view_count = item['statistics'].get('viewCount', None) #Allows me to see the history of the channel and how many views it has\n","                channel_made_for_kids = item['status'].get('madeForKids', None) #Allows me to see if the channel is made for kids\n","                country = item['snippet'].get('country', None) #In the search, I do set to try and find US videos but sometimes the channel is not from the country. So I search for this\n","                channel_publish_date = item['snippet'].get('publishedAt', None) #Was curious to understand if age of the channel or age of the account mattered\n","\n","\n","                # Append item to batch_rows\n","                batch_rows.append({\n","                    'Channel ID': channel_id,\n","                    'Subscriber Count': subscriber_count,\n","                    'Channel Views': channel_view_count,\n","                    'Channel Made for Kids': channel_made_for_kids,\n","                    'Country': country,\n","                    'Channel Published Date': channel_publish_date\n","                })\n","\n","        # Append to CSV after processing each batch of 50 videos\n","        if len(batch_rows) >= 50 or i == num_batches - 1:\n","            append_to_csv(batch_rows)\n","            batch_rows = []\n","\n","        # Print remaining videos\n","        remaining_channels = total_channels_remaining - (i + 1) * batch_size\n","        print(f\"Remaining videos to be processed: {remaining_channels}\")\n","\n","    print(\"All videos have been added to the CSV file.\")\n","\n","# Function to append to CSV\n","def append_to_csv(batch_rows):\n","    # Append DataFrame to CSV file\n","    if batch_rows:\n","        df = pd.DataFrame(batch_rows)\n","        mode = 'a' if pd.read_csv(video_info_file_path, nrows=1).empty else 'w'\n","        df.to_csv(video_info_file_path, mode=mode, index=False, header=mode=='w')\n","        print(\"Videos have been added to the CSV file.\")\n","\n","# Run the main function\n","start_time = time.time()\n","main()\n","end_time = time.time()\n","\n","# Total processing time\n","print(f\"Total processing time: {end_time - start_time} seconds.\")\n","\n","'''\n","Part 5\n","Goal: Finding the Game Played and Game Year for the Video\n","Description: Wanted to understand if the Game Played and the Game Year matters when having a video over 100k\n","Note: This has a big delay. Ran multiple processes for this to go faster but it did eventually work. Because it took a while, I added a part where it double checks a CSV to see what else needs to be added.\n","Output: Here is the csv it leads to https://drive.google.com/file/d/1yD3hD2rcFz27lKkCl00LyL2_UrXZbiub/view\n","'''\n","import pandas as pd\n","import requests\n","import time\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# File paths\n","search_results_file_path = '/content/drive/My Drive/Capstone Project 2/search_results_video_info_no_shorts.csv'\n","video_info_file_path = '/content/drive/My Drive/Capstone Project 2/video_activity_channel.csv'\n","\n","# Function to fetch data from URL synchronously with retries\n","def fetch_data_with_retries(url, retries=3, timeout=60):\n","    for _ in range(retries):\n","        try:\n","            print(f\"Fetching data from URL: {url}\")  # Print the URL being accessed\n","            response = requests.get(url, timeout=timeout)\n","            if response.status_code == 200:\n","                return response.json()\n","            elif response.status_code == 404:\n","                print(\"URL not found.\")\n","                return None\n","            elif response.status_code == 429:\n","                print(\"Rate limit exceeded. Waiting before retrying...\")\n","                time.sleep(60)  # Wait for a minute before retrying\n","            else:\n","                print(f\"Unexpected status code: {response.status_code}. Retrying...\")\n","                time.sleep(30)  # Adding a small delay before retrying\n","        except requests.Timeout:\n","            print(\"Timeout occurred. Retrying...\")\n","            time.sleep(30)  # Adding a small delay before retrying\n","        except Exception as e:\n","            print(f\"Error occurred: {e}. Retrying...\")\n","            time.sleep(30)  # Adding a small delay before retrying\n","    print(\"Failed after multiple retries. Skipping this batch.\")\n","    return None\n","\n","# Function to process all batches and append to CSV\n","def main():\n","    # Load CSV file containing video IDs\n","    search_results_df = pd.read_csv(search_results_file_path)\n","    # Extract video IDs\n","    video_ids_all = set(search_results_df['Video ID'].unique())\n","    # If there are no new video IDs, exit\n","    if not video_ids_all:\n","        print(\"No new video IDs to fetch.\")\n","        return\n","\n","    # Check the CSV file to see which videos still need to be processed\n","    processed_video_ids = set()\n","    try:\n","        processed_video_df = pd.read_csv(video_info_file_path)\n","        processed_video_ids = set(processed_video_df['Video ID'])\n","    except FileNotFoundError:\n","        pass\n","\n","    video_ids_remaining = list(video_ids_all - processed_video_ids)\n","    total_videos_remaining = len(video_ids_remaining)\n","    print(f\"Total videos remaining to be processed: {total_videos_remaining}\")\n","\n","    # Reverse the order of video IDs\n","    video_ids_remaining = video_ids_remaining[::-1]\n","\n","    # Asynchronously fetch data for remaining videos\n","    batch_rows = []\n","    for i in range(0, total_videos_remaining, 50):\n","        video_ids_batch = video_ids_remaining[i:i + 50]\n","        video_id_string_batch = ','.join(video_ids_batch)\n","        url = f\"https://yt4.lemnoslife.com/videos?part=activity&id={video_id_string_batch}\"\n","        data = fetch_data_with_retries(url)\n","\n","        if data is not None and 'items' in data:\n","            for item in data['items']:\n","                # Process each item and append to batch_rows\n","                video_id = item['id']\n","                activity = item.get('activity', {})\n","\n","                # Extract Published At, Description, and Title\n","                game_listed = activity.get('name', '')\n","                game_year = activity.get('year', '')\n","\n","                # Append item to batch_rows if video ID is not already in CSV\n","                if video_id not in processed_video_ids:\n","                    batch_rows.append({\n","                        'Video ID': video_id,\n","                        'Game Played': game_listed,#I found that this held what game was played. The creator sometimes had it blank though\n","                        'Game Year': game_year #Interested to know if year of the game mattered\n","                    })\n","                    processed_video_ids.add(video_id)\n","\n","        # Append to CSV after processing each batch of 50 videos\n","        if len(batch_rows) >= 50 or i + 50 >= total_videos_remaining:\n","            append_to_csv(batch_rows)\n","            batch_rows = []\n","\n","            # Print remaining videos\n","            remaining_videos = total_videos_remaining - (i + len(batch_rows))\n","            print(f\"Remaining videos to be processed: {remaining_videos}\")\n","\n","    print(\"All videos have been added to the CSV file.\")\n","\n","# Function to append to CSV\n","def append_to_csv(batch_rows):\n","    if batch_rows:\n","        df = pd.DataFrame(batch_rows)\n","        mode = 'a' if pd.read_csv(video_info_file_path, nrows=1).empty else 'w'\n","        df.to_csv(video_info_file_path, mode=mode, index=False, header=mode=='w')\n","        print(\"Videos have been added to the CSV file.\")\n","\n","# Run the main function\n","start_time = time.time()\n","main()\n","end_time = time.time()\n","\n","# Total processing time\n","print(f\"Total processing time: {end_time - start_time} seconds.\")\n","\n","'''\n","Part 6\n","Goal: Collecting past videos for each channel to better understand if a channel creator is consistent and see if past views matter\n","Description: For these past views, I collected numerous fields. The main fields I eventually focus on are the following\n","Likes: Trying to understand if past likes factor into understanding a good video\n","Views: Trying to understand if past\n","Videos: Trying to understand if you post videos consistently\n","Note: This also took a while but it definitely paid dividends in the end. Also, the file ends up being over 1 GB. I ran the code multiple times because sometimes it was someones first video ever posted. So had to run multiple times to confirm that was the case.\n","Output: Here is the csv it leads to https://drive.google.com/file/d/1Rafp0nQd6jGXO-VstrqAgqGMSRviW92a/view\n","'''\n","import csv\n","import pandas as pd\n","from datetime import datetime\n","import requests\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define the base URL template\n","base_url_template = \"https://yt4.lemnoslife.com/noKey/search?part=snippet&type=video&channelId={}&publishedAfter={}&publishedBefore={}&maxResults=50&pageToken={}\"\n","\n","# File paths\n","all_csv_file = '/content/drive/My Drive/Capstone Project 2/video_activity_channel.csv'\n","output_csv_file = '/content/drive/My Drive/Capstone Project 2/past_channel_videos.csv'\n","\n","# Read the CSV file containing all channel IDs\n","all_df = pd.read_csv(all_csv_file)\n","\n","# Filter out the unique channel IDs for which video counts are not already present in the existing CSV\n","unique_channel_ids = all_df['Channel ID'].unique()\n","unique_remaining_ids = unique_channel_ids\n","\n","# Define the start and end dates for the date range\n","start_date = datetime(2023, 11, 1).strftime('%Y-%m-%dT%H:%M:%SZ')\n","end_date = datetime(2024, 1, 29).strftime('%Y-%m-%dT%H:%M:%SZ')\n","\n","# Function to get the list of videos uploaded by a channel within the date range\n","def get_videos(channel_id):\n","    max_results = 50  # Maximum results per page\n","    page_token = ''   # Initialize page token to an empty string\n","\n","    all_videos_data = []  # List to store all video data\n","\n","    while True:  # Loop until all pages are fetched\n","        url = base_url_template.format(channel_id, start_date, end_date, page_token)\n","\n","        try:\n","            print(\"Fetching data for channel ID:\", channel_id)\n","            print(\"URL:\", url)  # Print the URL being used for each channel ID\n","            response = requests.get(url, timeout=10)  # Set timeout to 10 seconds\n","            response.raise_for_status()  # Raise an error for non-2xx status codes\n","            json_data = response.json()\n","\n","            # Extract video data from the current page\n","            for item in json_data.get('items', []):\n","                video_id = item['id']\n","                snippet = item.get('snippet', {})\n","                content_details = item.get('contentDetails', {})\n","                statistics = item.get('statistics', {})\n","                live_streaming_details = item.get('liveStreamingDetails', {})\n","                status = item.get('status', {})\n","\n","                channel_id = snippet.get('channelId', '')\n","                channel_title = snippet.get('channelTitle', '')\n","                tags = ','.join(snippet.get('tags', []))\n","                duration = content_details.get('duration', '')\n","                content_rating = content_details.get('contentRating', '')\n","                yt_rating = content_details.get('contentRating', {}).get('ytRating', '')\n","                made_for_kids = status.get('madeForKids', '')\n","                self_declared_made_for_kids = status.get('selfDeclaredMadeForKids', '')\n","                view_count = statistics.get('viewCount', '')\n","                like_count = statistics.get('likeCount', '')\n","                stream_start_time = live_streaming_details.get('actualStartTime', '')\n","\n","                # Append row to batch_rows list\n","                all_videos_data.append({\n","                    'Video ID': video_id,\n","                    'Channel ID': channel_id,\n","                    'Channel Title': channel_title,\n","                    'Tags': tags,\n","                    'Duration': duration,\n","                    'Content Rating': content_rating,\n","                    'YT Rating': yt_rating,\n","                    'Made For Kids': made_for_kids,\n","                    'Self Declared Made For Kids': self_declared_made_for_kids,\n","                    'View Count': view_count,\n","                    'Like Count': like_count,\n","                    'Stream Start Time': stream_start_time\n","                })\n","\n","            # Check if there are more pages to fetch\n","            next_page_token = json_data.get('nextPageToken')\n","            if not next_page_token:\n","                break  # Break the loop if no more pages\n","\n","            page_token = next_page_token  # Update page token for the next request\n","\n","        except requests.Timeout:\n","            print(f\"Timeout error: Request timed out for channel {channel_id}\")\n","            return channel_id, []\n","        except requests.RequestException as e:\n","            print(f\"Error retrieving data for channel {channel_id}: {e}\")\n","            return channel_id, []\n","\n","    return channel_id, all_videos_data\n","\n","\n","# Check if the output CSV file already exists\n","file_exists = False\n","\n","for idx, channel_id in enumerate(unique_remaining_ids, start=1):\n","    channel_id, videos_data = get_videos(channel_id)  # Unpack the tuple returned by get_videos\n","\n","    # Open the CSV file in append mode ('a') if it exists, otherwise open it in write mode ('w') to create it\n","    with open(output_csv_file, 'a' if file_exists else 'w', newline='', encoding='utf-8') as csvfile:\n","        fieldnames = ['Channel ID', 'Video ID', 'Published At','View Count','Like Count']\n","        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","\n","        # Write the header only if the file is being created for the first time\n","        if not file_exists:\n","            writer.writeheader()\n","            file_exists = True  # Set flag to indicate that the file now exists\n","\n","        for video_data in videos_data:\n","            writer.writerow(video_data)\n","\n","        remaining_ids = unique_remaining_ids[idx:]\n","        print(f\"[{idx}/{len(unique_remaining_ids)}] Video data for channel {channel_id} appended to channel_videos.csv. {len(remaining_ids)} unique channel IDs remain.\")\n","\n","print(\"Video data for all unique remaining channels appended to channel_videos.csv\")\n","'''\n","Part 7\n","Goal: Merging Video Info Captured, Channel Info Captured, Channel Stats Captured, and the Video Information\n","Description: I wanted to merge all the info into one place so I can then have a CSV where I can add new features where necessary\n","Note: Nov_Jan_Channel_Stats.csv is a shortened version of past_channel_videos. I did this because past_channel_videos was 1 gb.\n","Output: This will be the outputted csv. It is from another csv because I overwrote it\n","https://drive.google.com/file/d/1yD3hD2rcFz27lKkCl00LyL2_UrXZbiub/view\n","'''\n","\n","import csv\n","import pandas as pd\n","from datetime import datetime\n","import requests\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define the base URL template\n","base_url_template = \"https://yt4.lemnoslife.com/noKey/search?part=snippet&type=video&channelId={}&publishedAfter={}&publishedBefore={}&maxResults=50&pageToken={}\"\n","\n","# File paths\n","all_csv_file = '/content/drive/My Drive/Capstone Project 2/video_activity_channel.csv'\n","output_csv_file = '/content/drive/My Drive/Capstone Project 2/past_channel_videos.csv'\n","\n","# Read the CSV file containing all channel IDs\n","all_df = pd.read_csv(all_csv_file)\n","\n","# Filter out the unique channel IDs for which video counts are not already present in the existing CSV\n","unique_channel_ids = all_df['Channel ID'].unique()\n","unique_remaining_ids = unique_channel_ids\n","\n","# Define the start and end dates for the date range\n","start_date = datetime(2023, 11, 1).strftime('%Y-%m-%dT%H:%M:%SZ')\n","end_date = datetime(2024, 1, 29).strftime('%Y-%m-%dT%H:%M:%SZ')\n","\n","# Function to get the list of videos uploaded by a channel within the date range\n","def get_videos(channel_id):\n","    max_results = 50  # Maximum results per page\n","    page_token = ''   # Initialize page token to an empty string\n","\n","    all_videos_data = []  # List to store all video data\n","\n","    while True:  # Loop until all pages are fetched\n","        url = base_url_template.format(channel_id, start_date, end_date, page_token)\n","\n","        try:\n","            print(\"Fetching data for channel ID:\", channel_id)\n","            print(\"URL:\", url)  # Print the URL being used for each channel ID\n","            response = requests.get(url, timeout=10)  # Set timeout to 10 seconds\n","            response.raise_for_status()  # Raise an error for non-2xx status codes\n","            json_data = response.json()\n","\n","            # Extract video data from the current page\n","            for item in json_data.get('items', []):\n","                video_id = item['id']\n","                snippet = item.get('snippet', {})\n","                content_details = item.get('contentDetails', {})\n","                statistics = item.get('statistics', {})\n","                live_streaming_details = item.get('liveStreamingDetails', {})\n","                status = item.get('status', {})\n","\n","                channel_id = snippet.get('channelId', '')\n","                channel_title = snippet.get('channelTitle', '')\n","                tags = ','.join(snippet.get('tags', []))\n","                duration = content_details.get('duration', '')\n","                content_rating = content_details.get('contentRating', '')\n","                yt_rating = content_details.get('contentRating', {}).get('ytRating', '')\n","                made_for_kids = status.get('madeForKids', '')\n","                self_declared_made_for_kids = status.get('selfDeclaredMadeForKids', '')\n","                view_count = statistics.get('viewCount', '')\n","                like_count = statistics.get('likeCount', '')\n","                stream_start_time = live_streaming_details.get('actualStartTime', '')\n","\n","                # Append row to batch_rows list\n","                all_videos_data.append({\n","                    'Video ID': video_id,\n","                    'Channel ID': channel_id,\n","                    'Channel Title': channel_title,\n","                    'Tags': tags,\n","                    'Duration': duration,\n","                    'Content Rating': content_rating,\n","                    'YT Rating': yt_rating,\n","                    'Made For Kids': made_for_kids,\n","                    'Self Declared Made For Kids': self_declared_made_for_kids,\n","                    'View Count': view_count,\n","                    'Like Count': like_count,\n","                    'Stream Start Time': stream_start_time\n","                })\n","\n","            # Check if there are more pages to fetch\n","            next_page_token = json_data.get('nextPageToken')\n","            if not next_page_token:\n","                break  # Break the loop if no more pages\n","\n","            page_token = next_page_token  # Update page token for the next request\n","\n","        except requests.Timeout:\n","            print(f\"Timeout error: Request timed out for channel {channel_id}\")\n","            return channel_id, []\n","        except requests.RequestException as e:\n","            print(f\"Error retrieving data for channel {channel_id}: {e}\")\n","            return channel_id, []\n","\n","    return channel_id, all_videos_data\n","\n","\n","# Check if the output CSV file already exists\n","file_exists = False\n","\n","for idx, channel_id in enumerate(unique_remaining_ids, start=1):\n","    channel_id, videos_data = get_videos(channel_id)  # Unpack the tuple returned by get_videos\n","\n","    # Open the CSV file in append mode ('a') if it exists, otherwise open it in write mode ('w') to create it\n","    with open(output_csv_file, 'a' if file_exists else 'w', newline='', encoding='utf-8') as csvfile:\n","        fieldnames = ['Channel ID', 'Video ID', 'Published At','View Count','Like Count']\n","        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","\n","        # Write the header only if the file is being created for the first time\n","        if not file_exists:\n","            writer.writeheader()\n","            file_exists = True  # Set flag to indicate that the file now exists\n","\n","        for video_data in videos_data:\n","            writer.writerow(video_data)\n","\n","        remaining_ids = unique_remaining_ids[idx:]\n","        print(f\"[{idx}/{len(unique_remaining_ids)}] Video data for channel {channel_id} appended to channel_videos.csv. {len(remaining_ids)} unique channel IDs remain.\")\n","\n","print(\"Video data for all unique remaining channels appended to channel_videos.csv\")\n","\n","'''\n","Part 8\n","Goal: Create new features to better understand how a good youtube video is made. Also filled in null values\n","Description: Created a variety of new features while also filling in null values. For filling null values, this is what I did:\n","If Channel Views were null, I filled it in with the View Count of the current video\n","If the Channel Country was null, I filled it in with US\n","If Channel Published Date was null, I filled it in with Published at date due to the amount of nulls being low\n","If any past channel stats were null, I filled it in as 0 because they had no videos\n","For Game Played, if it was null, I tried to look at title and tags to fill in the correct game. If it was null, I kept it that way because it usually was a rare game or it actually ended up not being a game. I also made games the same name where necessary\n","For Game Year, I filled in for any games that I already had or that I knew\n","Used Google Trends to find where a trending word was located in the video details. Separated it into Top Terms and Rising Words because Google trends has each\n","Found top games in (https://steamdb.info/). I looked at top games sold from Jan 15th to Feb 15th because those were the games that would potentially be popular\n","Found Top Selling Games for 2024 (https://www.gamespot.com/gallery/2024s-best-selling-games-in-the-us/2900-5106/) to understand if people played these games,would they do better\n","Found the Avg Views, Likes and Videos for last 3 months to understand how popular the channel was in the past. Also found the percentage of likes for past videos to better understand if the videos have been consistently popular over time\n","Found the hour that the video got launched using EST to see if that matters. Also put hours into different blocks to see if that mattered\n","Put Day of Week to better understand if launching on a certain day mattered\n","I found channel age to see if older channels would do better because they are around longer\n","I wanted to see how a video was launched, so I double checked to know if it used the premier feature or if it was a stream\n","I checked what kind of Social Media links used in the description to see if Promoting their own videos mattered\n","For tags, I checked if they existed and how many tags you would have to see if adding more tags would mean more views and if tags mattered at all\n","For a description, I was trying to understand if that mattered and found if a description mattered or not\n","Finally, I put a number code to everything so it would work for a linear regression model\n","Output: Here is the CSV it gets outputted to\n","'''\n","import pandas as pd\n","import numpy as np\n","from pytz import timezone\n","\n","#Creating df of all videos together\n","activity_df = pd.read_csv(\"C:/Users/sulli/Downloads/Youtube Folder/video_activity_channel.csv\")\n","\n","#If there were no channel views.More likely than not, it was there first video. Hence filling it in with view count\n","activity_df['Channel Views'] = activity_df['Channel Views'].fillna(activity_df['View Count'])\n","activity_df['Country'] = activity_df['Country'].fillna('US')\n","\n","# Convert boolean values to True/False\n","activity_df['Channel Made for Kids'] = activity_df['Channel Made for Kids'].astype(bool)\n","activity_df['Video Captions'] = activity_df['Video Captions'].astype(bool)\n","activity_df['Video Licensed'] = activity_df['Video Licensed'].astype(bool)\n","\n","# Convert NaN values to False\n","activity_df['Channel Made for Kids'] = activity_df['Channel Made for Kids'].fillna(False)\n","activity_df['Video Captions'] = activity_df['Video Captions'].fillna(False)\n","activity_df['Video Licensed'] = activity_df['Video Licensed'].fillna(False)\n","\n","\n","#This is from the Nov_Jan_Channel CSV\n","activity_df['Channel Published Date'] = activity_df['Channel Published Date'].fillna(activity_df['Published At'])\n","fields_to_fill = [\n","    'January Views', 'November Views', 'December Views',\n","    'January Likes', 'November Likes', 'December Likes',\n","    'January Videos', 'November Videos', 'December Videos',\n","    'Last Two Months Views', 'Last Two Months Likes', 'Last Two Months Videos',\n","    'Last Three Months Views', 'Last Three Months Likes', 'Last Three Months Videos',\n","    'Subscriber Count','Like Count'\n","]\n","\n","\n","\n","\n","# Fill all specified fields with 0\n","activity_df[fields_to_fill] = activity_df[fields_to_fill].fillna(0)\n","#I filled in the game played where I could based on a bunch of factors you will see below. If it was null, I still kept the videos\n","activity_df['Game Played Corrected'] = activity_df['Game Played']\n","activity_df['Game Played Corrected'] = activity_df['Game Played'].str.title()\n","activity_df['Game Year Corrected'] = activity_df['Game Year']\n","\n","\n","# If game exists in title or tags, fill it in.\n","\n","unique_games = activity_df['Game Played Corrected'].dropna().unique()  # Get unique non-null games\n","\n","# Function to find if any game exists in the tags\n","def find_matching_game(tags):\n","    matched_games = []\n","    for game in unique_games:\n","        if game.lower() in tags.lower():\n","            matched_games.append(game)\n","            if len(matched_games) > 1:\n","                return None  # More than one game found in tags, return None\n","    if len(matched_games) == 1:\n","        return matched_games[0]  # Return the single matched game\n","    return None  # No game found in tags, return None\n","\n","activity_df['Game Played Corrected'] = activity_df['Game Played Corrected'].str.replace('®', '')\n","activity_df['Game Played Corrected'] = activity_df['Game Played Corrected'].str.replace('Ã©', 'é')\n","activity_df['Game Played Corrected'] = activity_df['Game Played Corrected'].str.replace('GTA', 'Grand Theft Auto')\n","\n","name_mapping = {\n","    'UNO!™': 'Uno',\n","    'The Simpsons': 'The Simpsons Game',\n","    'GTA: San Andreas - Definitive': 'Grand Theft Auto: San Andreas - The Definitive Edition',\n","    'Call of Duty: Modern Warfare III': 'Call of Duty: Modern Warfare 3',\n","    'Call of Duty: Modern Warfare II': 'Call of Duty: Modern Warfare 2'\n","\n","}\n","\n","# Replace values in the 'Game Played' column using the defined mapping\n","activity_df['Game Played Corrected'] = activity_df['Game Played Corrected'].replace(name_mapping)\n","#I looked at what was empty and created this to fill in the game where I could along with what was popular and missing\n","for index, row in activity_df.iterrows():\n","    print(f\"Processing row {index}...\")\n","    if pd.isnull(row['Game Played Corrected']):  # Debug statement\n","        title_value = row['Video Title']\n","        print(f\"Title Value: {title_value}\")\n","        tags_value = row['Tags']\n","        print(f\"Tags Value: {tags_value}\")\n","        if isinstance(title_value, str):  # Check if 'Video Title' is a string\n","            print(\"Title is a string.\")\n","            title_value = title_value.title()# Apply .title() to the string\n","            title_value = title_value.str.replace('®', '')\n","            print(f\"Modified Title Value: {title_value}\")\n","            for game in unique_games:\n","                if str(game) in title_value:  # Convert game to string before checking\n","                    print(f\"Game {game} found in title.\")\n","                    activity_df.at[index, 'Game Played Corrected'] = game\n","                    print(f\"Game Played Corrected set to: {game}\")\n","                    break  # Stop searching if a match is found\n","\n","            print(f\"Title Value (lowercase) after for loop: {title_value.lower()}\")\n","            if 'madden' in title_value.lower() and '24' in title_value.lower():\n","                print(\"Performing check for 'Madden NFL 24\")\n","                print(f\"Game Played Corrected set to: {game}\")\n","                activity_df.at[index, 'Game Played Corrected'] = 'Madden NFL 24'\n","            elif 'buckshot roulette' in title_value.lower():\n","                print(\"Performing check for 'Buckshot\")\n","                activity_df.at[index, 'Game Played Corrected'] = 'Buckshot Roulette'\n","                print(f\"Game Played Corrected set to: {game}\")\n","            elif 'mw3' in title_value.lower() or 'modern warfare iii' in title_value.lower() or 'modern warfare 3' in title_value.lower():\n","                print(\"Performing check for 'mw3\")\n","                activity_df.at[index, 'Game Played Corrected'] = 'Call of Duty: Modern Warfare 3'\n","                print(f\"Game Played Corrected set to: {game}\")\n","            elif 'mw2' in title_value.lower():\n","                print(\"Performing check for 'mw2\")\n","                activity_df.at[index, 'Game Played Corrected'] = 'Call of Duty: Modern Warfare 2'\n","                print(f\"Game Played Corrected set to: {game}\")\n","            elif 'black ops 2' in title_value.lower():\n","                print(\"Performing check for 'black ops 2\")\n","                activity_df.at[index, 'Game Played Corrected'] = 'Call of Duty: Black Ops II'\n","                print(f\"Game Played Corrected set to: {game}\")\n","            elif 'black ops cold war' in title_value.lower():\n","                print(\"Performing check for 'cold war\")\n","                activity_df.at[index, 'Game Played Corrected'] = 'Call of Duty: Black Ops Cold War'\n","                print(f\"Game Played Corrected set to: {game}\")\n","            elif 'gta 5' in title_value.lower():\n","                print(\"Performing check for 'gta 5\")\n","                activity_df.at[index, 'Game Played Corrected'] = 'Grand Theft Auto V'\n","                print(f\"Game Played Corrected set to: {game}\")\n","            elif 'ww2' in title_value.lower() and 'cod' in title_value.lower():\n","                print(\"Performing check for 'wwii\")\n","                activity_df.at[index, 'Game Played Corrected'] = 'Call of Duty: WWII'\n","                print(f\"Game Played Corrected set to: {game}\")\n","            elif 'wwii' in title_value.lower() and 'cod' in title_value.lower():\n","                print(\"Performing check for 'wwii\")\n","                activity_df.at[index, 'Game Played Corrected'] = 'Call of Duty: WWII'\n","                print(f\"Game Played Corrected set to: {game}\")\n","            elif 'ww2' in title_value.lower() and 'call of' in title_value.lower():\n","                print(\"Performing check for 'wwii\")\n","                activity_df.at[index, 'Game Played Corrected'] = 'Call of Duty: WWII'\n","                print(f\"Game Played Corrected set to: {game}\")\n","            elif 'wwii' in title_value.lower() and 'call of' in title_value.lower():\n","                print(\"Performing check for 'wwii\")\n","                activity_df.at[index, 'Game Played Corrected'] = 'Call of Duty: WWII'\n","            elif 'call of' in title_value.lower() and 'mobile' in title_value.lower():\n","                print(\"Performing check for 'Call of Duty: Mobile'...\")  # New print statement\n","                activity_df.at[index, 'Game Played Corrected'] = 'Call of Duty: Mobile'\n","                print(f\"Game Played Corrected set to: Call of Duty: Mobile\")\n","            elif 'fc 24' in title_value.lower() or 'fc24' in title_value.lower():\n","                print(\"Performing check for 'fc24\")\n","                activity_df.at[index, 'Game Played Corrected'] = 'EA Sports FC 24'\n","                print(f\"Game Played Corrected set to: {game}\")\n","\n","        else:  # Check if any game from unique_games is in tags\n","            if isinstance(tags_value, str):  # Check if 'Tags' is a string\n","                tags_value = tags_value.title()  # Apply .title() to the string\n","                activity_df.at[index, 'Game Played Corrected'].fillna(find_matching_game(tags_value), inplace=True)\n","                if 'nopixel' in tags_value.lower():\n","                    activity_df.at[index, 'Game Played Corrected'] = 'Grand Theft Auto V'\n","                elif 'madden 24' in tags_value.lower():\n","                    activity_df.at[index, 'Game Played Corrected'] = 'Madden NFL 24'\n","\n","'''\n","Part 9\n","Goal: Trying to see which model is better\n","Description: I printed out all the models and imported them into the following Google Sheet. Here are the Important Tabs:\n","Fields: Explaining the name of each field and then categorizing them into why they are important. I use this to ensure I am ideally using all the categories to come up with a good model\n","Feature Importance (EX): Understanding which features were important so I know what to tweak. For “All Features” Model, this was my way to understand which features were important and knowing what to keep. For each version I tried to keep the top features per categories. In my model names, it explains what it was doing\n","Compare 3 (Classification): For each model, I captured which ones had the best metrics. For Overall Score, it is a weighted average of Precision, Recall, F1 Score, and ROC AUC. I weighted Precision and ROC AUC higher because, if I received a FP, it would be worse than receiving an FN. I want to be conservative and not predict a video will hit 100k and it won’t\n","Note: I kept Regression metrics as well because I thought I had a regression problem. But, because I want to know if a video hits 100k or not, it is a Classification problem. Hence, Classification metrics were used. I also tweaked the models to better understand the features to better understand the relationship between features.\n","What Model did I choose and Why: I ended up choosing a model with 10 features. It wasn’t the best performing but it was only worse than a model with 16 features by .11%. Because it would take less computational power, I stuck with a model with 10 features.  I chose XGBoost because it performed the best overall. Decision Tree was a close second but XGBoost performed better overall\n","Output: I put all coefficients/feature importance into CSVs, then would put them in this Sheet to better understand what models were better and why\n","(https://docs.google.com/spreadsheets/d/14jlshDILuqAhJBXq2VUmHmBKPk5Fhhhka_UIY8qREiI/edit#gid=201532733)\n","'''\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.linear_model import LinearRegression\n","import statsmodels.api as sm\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from google.colab import drive\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from xgboost import XGBRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, roc_auc_score, precision_score, recall_score, f1_score\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","\n","\n","# Load dataset\n","model_df = pd.read_csv(\"/content/drive/My Drive/Capstone Project 2/video_activity_channel_altered.csv\")\n","\n","# Define categorical and continuous features\n","categorical_features = ['Channel ID', 'Channel Title', 'Tags', 'Made For Kids', 'Stream End Time', 'Published At',\n","                        'Video Title', 'Video Description', 'Video Definition', 'Video Captions', 'Video Licensed',\n","                        'Video Projection', 'Video Dimension', 'Game Played', 'Game Year', 'Channel Made for Kids',\n","                        'Country', 'Channel Published Date','Game Played Corrected','Game Year Corrected',\n","                        'Top Search Term in Title','Top Search Term in Game Listed','Top Search Term in Description',\n","                        'Top Search Term in Tags','YouTube Rising Word in Title','YouTube Rising Word in Game Listed',\n","                        'YouTube Rising Word in Description','YouTube Rising Word in Tags','Top Selling Steam Game',\n","                        'Top Selling Game 2024','Day of Week','Time of Day', 'Channel Age', 'Hour','Video Definition Code',\n","                        'Video Projection Code','Video Dimension Code','Game Played Corrected Code','Country Code',\n","                        'Day of Week Code','Time of Day Code','Links Twitter','Links Instagram','Links Facebook',\n","                        'Links Twitch','Premier Stream Video Code','Primary Social Media Link','Tags Exist',\n","                        'Video Licensed Code','Description Exist','Game Played Listed in Activity']\n","continuous_features = ['View Count', 'Like Count', 'duration_seconds', 'Subscriber Count', 'Channel Views',\n","                        'January Views','November Views','December Views','January Likes','November Likes','December Likes',\n","                        'January Videos','November Videos','December Videos','Last Two Months Views','Last Two Months Likes',\n","                        'Last Two Months Videos','Last Three Months Views','Last Three Months Likes','Last Three Months Videos',\n","                        'View Count Percentile','Avg Views Last 3 Months','Avg Views Last 3 Months','Avg Views Last 2 Months',\n","                        'Avg Views Last 1 Month','Amount of Tags','Avg Likes Last 3 Months','Avg Likes Last 2 Months',\n","                        'Avg Likes Last 1 Month','Likes to Views Ratio','Likes to Views Ratio Last 3 Month',\n","                        'Likes to Views Ratio Last 2 Month','Likes to Views Ratio Last 1 Month']\n","# Define the versions with different sets of features\n","versions = [\n","    {\n","        'name': 'All Features',\n","        'features': ['Like Count', 'duration_seconds', 'Subscriber Count', 'Channel Views',\n","                        'January Views','November Views','December Views','January Likes','November Likes','December Likes',\n","                        'January Videos','November Videos','December Videos','Last Two Months Views','Last Two Months Likes',\n","                        'Last Two Months Videos','Last Three Months Views','Last Three Months Likes','Last Three Months Videos',\n","                        'Avg Views Last 3 Months','Avg Views Last 2 Months', 'Avg Views Last 1 Month','Made For Kids',\n","                        'Video Definition Code', 'Video Captions', 'Video Licensed Code',\n","                        'Video Projection Code', 'Video Dimension Code', 'Channel Made for Kids',\n","                        'Country Code', 'Game Played Corrected Code','Game Year Corrected',\n","                        'Top Search Term in Title','Top Search Term in Game Listed','Top Search Term in Description',\n","                        'Top Search Term in Tags','YouTube Rising Word in Title','YouTube Rising Word in Game Listed',\n","                        'YouTube Rising Word in Description','YouTube Rising Word in Tags','Top Selling Steam Game',\n","                        'Top Selling Game 2024','Day of Week Code','Time of Day Code', 'Channel Age', 'Hour',\n","                        'Links Twitter','Links Instagram','Links Facebook','Links Twitch','Primary Social Media Link',\n","                        'Tags Exist','Amount of Tags','Premier Stream Video Code','Description Exist',\n","                        'Game Played Listed in Activity','Avg Likes Last 3 Months','Avg Likes Last 2 Months',\n","                        'Avg Likes Last 1 Month','Likes to Views Ratio','Likes to Views Ratio Last 3 Month',\n","                        'Likes to Views Ratio Last 2 Month','Likes to Views Ratio Last 1 Month']\n","    },\n","    {\n","        'name': 'Top Two Important Features from each category (XGB Boost)',\n","        'features': ['Like Count','Likes to Views Ratio','Video Definition Code','Video Licensed Code','Top Selling Game 2024',\n","                     'Time of Day Code','Links Twitter','Channel Age','Tags Exist','Premier Stream Video Code',\n","                     'Video Projection Code','Channel Made for Kids','Country Code','Links Instagram','Links Twitch',\n","                     'Amount of Tags']\n","    },\n","    {\n","        'name': 'Top Two Important Features from each category (Random Forest)',\n","        'features': ['Like Count','Avg Views Last 2 Months','duration_seconds','Channel Age','Day of Week Code','Hour',\n","                     'Amount of Tags','Game Played Corrected Code','Country Code','Channel Made for Kids',\n","                     'Primary Social Media Link','Links Twitch','Video Projection Code','Video Licensed Code']\n","    },\n","    {\n","        'name': 'Top Two Important Features from each category (Decision Tree)',\n","        'features': ['Like Count','Likes to Views Ratio','duration_seconds','Channel Age','Game Played Corrected Code','Hour',\n","                     'Day of Week Code','Amount of Tags','Country Code','Links Instagram','Made For Kids',\n","                     'Video Definition Code','Video Projection Code','Video Dimension Code','Channel Made for Kids']\n","    },\n","    {\n","        'name': 'Top Two Important Features from each category (Linear Regression)',\n","        'features': ['Video Projection Code','Video Licensed Code','Description Exist','Links Facebook','Links Twitter',\n","                     'Tags Exist','Video Definition Code','Hour','Day of Week Code','Made For Kids','Like Count',\n","                     'November Videos','Channel Made for Kids']\n","    },\n","    {\n","        'name': 'Top 1 Important Features from each category (XGB Boost)',\n","        'features': ['Like Count','Video Definition Code','Video Licensed Code','Time of Day Code','Links Twitter',\n","                     'Channel Age','Tags Exist','Channel Made for Kids','Country Code']\n","    },\n","    {\n","        'name': 'Top 1 Important Features from each category (Random Forest)',\n","        'features': ['Like Count','duration_seconds','Channel Age','Day of Week Code','Hour','Amount of Tags',\n","                     'Country Code','Links Twitter','Video Definition Code']\n","    },\n","    {\n","        'name': 'Top 1 Important Features from each category (Decision Tree)',\n","        'features': ['Like Count','duration_seconds','Channel Age','Hour','Amount of Tags','Country Code','Links Instagram',\n","                     'Video Definition Code']\n","    },\n","    {\n","        'name': 'Top 1 Important Features from each category (Linear Regression)',\n","        'features': ['Video Projection Code','Video Licensed Code','Links Facebook','Tags Exist','Hour',\n","                     'Made For Kids','Like Count','YouTube Rising Word in Title']\n","    },\n","    {\n","        'name': 'Tweaking 2 Important Features from Decision Tree',\n","        'features': ['Like Count','Likes to Views Ratio','Avg Views Last 3 Months','duration_seconds','Video Licensed Code',\n","                     'Game Played Corrected Code', 'Primary Social Media Link','Premier Stream Video Code','Time of Day Code',\n","                    'Top Selling Game 2024']\n","    },\n","    {\n","        'name': 'Top Coefficients from Each Category',\n","        'features': ['Video Projection Code', 'Primary Social Media Link', 'Made For Kids', 'Like Count',\n","                     'Top Search Term in Description', 'Channel Age', 'Avg Views Last 1 Month']\n","    }\n","]\n","\n","# Initialize lists to store results for each version\n","results = []\n","all_coefficients = []\n","\n","# Iterate over versions and train linear regression models\n","for version in versions:\n","    # Define features and target variable\n","    X = model_df[version['features']]\n","    y = model_df['Views']\n","\n","    # Split the dataset into train and test sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # Train the linear regression model\n","    model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n","\n","'''\n","Part 10\n","Goal: Trying to see which parameters to use. Then based on best parameters, show the final model I will use\n","Description: The script would loop through about 6300 permutations to see the best parameters. All parameters would be stored into a CSV which I would then look into and calculate what the best parameters would be to use. Similar to what model I chose, I weighted Precision, and ROC AUC as 30% and Recall + F1 score as 20%. Based on the weightage I chose what parameters I would be using\n","Output: Here is the way I decided what parameters to use. It improved every metric with ROC AUC seeing the biggest jump. The parameters found in Row 2 are the ones I ended up using\n","\n","\n","'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from xgboost import XGBRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","from sklearn.model_selection import ParameterGrid\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","\n","# Load your dataset\n","model_df = pd.read_csv(\"/content/drive/My Drive/Youtube_Folder/video_activity_channel_altered.csv\")\n","\n","# Decided on these features because it is a smaller amount while also testing for different aspects of a youtube video\n","feature_set =  ['Like Count','Likes to Views Ratio','Avg Views Last 3 Months','duration_seconds','Video Licensed Code',\n","                     'Game Played Corrected Code', 'Primary Social Media Link','Premier Stream Video Code','Time of Day Code',\n","                    'Top Selling Game 2024']\n","\n","# Define the hyperparameter grid\n","param_grid = {\n","    'learning_rate': [0.01, 0.1, 0.2],\n","    'max_depth': [3, 4, 5],\n","    'min_child_weight': [1, 2, 3],\n","    'subsample': [0.6, 0.8, 1.0],\n","    'colsample_bytree': [0.6, 0.8, 1.0],\n","    'gamma': [0, 0.1, 0.2],\n","    'reg_alpha': [0, 0.1, 0.5],\n","    'reg_lambda': [0, 0.1, 0.5]\n","}\n","\n","# Initialize lists to store results\n","results = []\n","feature_importance_results = []\n","\n","# Initialize XGBoost regressor\n","xgb_reg = XGBRegressor(random_state=42)\n","\n","# Iterate over hyperparameter tests\n","for i, params in enumerate(ParameterGrid(param_grid)):\n","    print(f\"Testing Hyperparameter Set {i + 1}/{len(ParameterGrid(param_grid))}\")\n","\n","    # Train-test split\n","    X_train, X_test, y_train, y_test = train_test_split(model_df[feature_set], model_df['Views'], test_size=0.2, random_state=42)\n","\n","    # Initialize XGBoost regressor with current hyperparameters\n","    xgb_reg.set_params(**params)\n","\n","    # Train the model\n","    xgb_reg.fit(X_train, y_train)\n","\n","    # Make predictions (continuous)\n","    y_pred_continuous = xgb_reg.predict(X_test)\n","\n","    # Convert to binary predictions using a threshold (e.g., 0.5)\n","    y_pred_binary = (y_pred_continuous > 0.5).astype(int)\n","\n","    # Calculate evaluation metrics for regression\n","    mae = mean_absolute_error(y_test, y_pred_continuous)\n","    mse = mean_squared_error(y_test, y_pred_continuous)\n","    r2 = r2_score(y_test, y_pred_continuous)\n","\n","    # Calculate evaluation metrics for classification\n","    accuracy = accuracy_score(y_test, y_pred_binary)\n","    precision = precision_score(y_test, y_pred_binary)\n","    recall = recall_score(y_test, y_pred_binary)\n","    f1 = f1_score(y_test, y_pred_binary)\n","    roc_auc = roc_auc_score(y_test, y_pred_continuous)\n","\n","    # Store results for both regression and classification metrics\n","    results.append({\n","        'Hyperparameters': params,\n","        'MAE': mae,\n","        'MSE': mse,\n","        'R2': r2,\n","        'Accuracy': accuracy,\n","        'Precision': precision,\n","        'Recall': recall,\n","        'F1 Score': f1,\n","        'ROC AUC': roc_auc\n","    })\n","\n","    print(f\"  MAE: {mae}, MSE: {mse}, R2: {r2}\")\n","    print(f\"  Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}, ROC AUC: {roc_auc}\")\n","\n","# Convert results to DataFrame\n","results_df = pd.DataFrame(results)\n","\n","results_df.to_csv(\"/content/drive/My Drive/Youtube_Folder/xgboost_results_for_parameters.csv\", index=False)\n","\n","# Display results\n","print(results_df)\n","\n","# Here is the model I chose based on number of features and based on the metrics I used to measure performance\n","\n","# Selected features\n","feature_set = ['Like Count', 'Likes to Views Ratio', 'Avg Views Last 3 Months', 'duration_seconds',\n","               'Video Licensed Code', 'Game Played Corrected Code', 'Primary Social Media Link',\n","               'Premier Stream Video Code', 'Time of Day Code', 'Top Selling Game 2024']\n","\n","# Selected hyperparameters\n","params = {\n","    'colsample_bytree': 1.0, #Sampling each and every feature for all samples\n","    'gamma': 0, #Essentially no regularization being used\n","    'learning_rate': 0.2, #Because it is a lower value, it makes the model more robust by shrinking the weights on each step\n","    'max_depth': 5, #5 decisions are made before it comes to a conclusion on if the video will be higher/lower than 100k views\n","    'min_child_weight': 2, #Each child must have 2 instances. This will hopefully prevent overfitting\n","    'reg_alpha': 0, # no L1 regularization is applied to the weights of the model.\n","    'reg_lambda': 0.5,#a moderate level of L2 regularization. It strikes a balance between preventing overfitting and allowing the model to learn from the data effectively.\n","    'subsample': 1.0 #All samples are being used to validate the model\n","}\n","\n","# Initialize XGBoost regressor with specified hyperparameters\n","xgb_reg = XGBRegressor(**params, random_state=42)\n","\n","# Train the model\n","xgb_reg.fit(model_df[feature_set], model_df['Views'])\n","\n","# Print the model\n","print(xgb_reg)\n","\n","#For xg boost, these parameters end up being the best: {'colsample_bytree': 1.0, 'gamma': 0, 'learning_rate': 0.2, 'max_depth': 5, 'min_child_weight': 2, 'reg_alpha': 0, 'reg_lambda': 0.5, 'subsample': 1.0}; So I go with this model\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]}]}