{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNSnc9jXlUauhFOHKAtGoFQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":741,"output_embedded_package_id":"1rv5izJ8foUq9Q8tXuMeiWnEjN8ltDlm_"},"id":"HCRHOvmL-DZD","executionInfo":{"status":"error","timestamp":1709582564430,"user_tz":480,"elapsed":14973,"user":{"displayName":"Patrick Sullivan","userId":"07409751838678152568"}},"outputId":"3accbdb2-8d64-466b-869b-11f3b71669a3"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import pandas as pd\n","import requests\n","import time\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# File paths\n","search_results_file_path = '/content/drive/My Drive/Capstone Project 2/search_results_video_info_no_shorts.csv'\n","video_info_file_path = '/content/drive/My Drive/Capstone Project 2/search_results_video_info_no_shorts_with_channel_info.csv'\n","\n","# Function to fetch data from URL synchronously with retries\n","def fetch_data_with_retries(url, retries=3):\n","    for _ in range(retries):\n","        try:\n","            print(f\"Fetching data from URL: {url}\")  # Print the URL being accessed\n","            response = requests.get(url)\n","            if response.status_code == 200:\n","                return response.json()\n","            else:\n","                print(f\"Request failed. Retrying...\")\n","                time.sleep(1)  # Adding a small delay before retrying\n","        except Exception as e:\n","            print(f\"Error occurred: {e}. Retrying...\")\n","            time.sleep(1)  # Adding a small delay before retrying\n","    print(\"Failed after multiple retries. Skipping this batch.\")\n","    return None\n","\n","# Function to process all batches and append to CSV\n","def main():\n","    # Load CSV file containing new channel IDs\n","    search_results_df = pd.read_csv(search_results_file_path)\n","    # Extract channel IDs\n","    channel_ids_all = set(search_results_df['Channel ID'].unique())\n","    # If there are no new video IDs, exit\n","    if not channel_ids_all:\n","        print(\"No new channel IDs to fetch.\")\n","        return\n","\n","    # Check the CSV file to see which videos still need to be processed\n","    try:\n","        processed_channel_df = pd.read_csv(video_info_file_path)\n","    except FileNotFoundError:\n","        processed_channel_df = pd.DataFrame()\n","\n","    processed_channel_ids = set(processed_channel_df['Channel ID']) if not processed_channel_df.empty else set()\n","    channel_ids_remaining = channel_ids_all - processed_channel_ids\n","\n","    total_channels_remaining = len(channel_ids_remaining)\n","    print(f\"Total channels remaining to be processed: {channel_ids_remaining}\")\n","\n","    # Determine the batch size\n","    batch_size = min(total_channels_remaining, 50)\n","    # Calculate the total number of batches\n","    num_batches = ((total_channels_remaining - 1) // batch_size) + 1\n","\n","    # Fetch data for remaining videos\n","    batch_rows = []\n","    for i in range(num_batches):\n","        start_index = i * batch_size\n","        end_index = min((i + 1) * batch_size, len(channel_ids_remaining))\n","        channel_ids_batch = list(channel_ids_remaining)[start_index:end_index]\n","        channel_id_string_batch = ','.join(channel_ids_batch)\n","        url = f\"https://yt4.lemnoslife.com/noKey/channels?part=snippet,statistics,status&id={channel_id_string_batch}\"\n","        data = fetch_data_with_retries(url)\n","\n","        if data is not None and 'items' in data:\n","            for item in data['items']:\n","                # Process each item and append to batch_rows\n","                channel_id = item['id']\n","                subscriber_count = item['statistics'].get('subscriberCount', None) #Allows me to see sub count\n","                channel_view_count = item['statistics'].get('viewCount', None) #Allows me to see the history of the channel and how many views it has\n","                channel_made_for_kids = item['status'].get('madeForKids', None) #Allows me to see if the channel is made for kids\n","                country = item['snippet'].get('country', None) #In the search, I do set to try and find US videos but sometimes the channel is not from the country. So I search for this\n","                channel_publish_date = item['snippet'].get('publishedAt', None) #Was curious to understand if age of the channel or age of the account mattered\n","\n","\n","                # Append item to batch_rows\n","                batch_rows.append({\n","                    'Channel ID': channel_id,\n","                    'Subscriber Count': subscriber_count,\n","                    'Channel Views': channel_view_count,\n","                    'Channel Made for Kids': channel_made_for_kids,\n","                    'Country': country,\n","                    'Channel Published Date': channel_publish_date\n","                })\n","\n","        # Append to CSV after processing each batch of 50 videos\n","        if len(batch_rows) >= 50 or i == num_batches - 1:\n","            append_to_csv(batch_rows)\n","            batch_rows = []\n","\n","        # Print remaining videos\n","        remaining_channels = total_channels_remaining - (i + 1) * batch_size\n","        print(f\"Remaining videos to be processed: {remaining_channels}\")\n","\n","    print(\"All videos have been added to the CSV file.\")\n","\n","# Function to append to CSV\n","def append_to_csv(batch_rows):\n","    # Append DataFrame to CSV file\n","    if batch_rows:\n","        df = pd.DataFrame(batch_rows)\n","        mode = 'a' if pd.read_csv(video_info_file_path, nrows=1).empty else 'w'\n","        df.to_csv(video_info_file_path, mode=mode, index=False, header=mode=='w')\n","        print(\"Videos have been added to the CSV file.\")\n","\n","# Run the main function\n","start_time = time.time()\n","main()\n","end_time = time.time()\n","\n","# Total processing time\n","print(f\"Total processing time: {end_time - start_time} seconds.\")\n","\n"]}]}