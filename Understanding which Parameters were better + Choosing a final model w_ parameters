{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNTceZu6lnSQKavg5wRx7jZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vmPc5wfulDIL"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from xgboost import XGBRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","from sklearn.model_selection import ParameterGrid\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","\n","# Load your dataset\n","model_df = pd.read_csv(\"/content/drive/My Drive/Youtube_Folder/video_activity_channel_altered.csv\")\n","\n","# Decided on these features because it is a smaller amount while also testing for different aspects of a youtube video\n","feature_set =  ['Like Count','Likes to Views Ratio','Avg Views Last 3 Months','duration_seconds','Video Licensed Code',\n","                     'Game Played Corrected Code', 'Primary Social Media Link','Premier Stream Video Code','Time of Day Code',\n","                    'Top Selling Game 2024']\n","\n","# Define the hyperparameter grid\n","param_grid = {\n","    'learning_rate': [0.01, 0.1, 0.2],\n","    'max_depth': [3, 4, 5],\n","    'min_child_weight': [1, 2, 3],\n","    'subsample': [0.6, 0.8, 1.0],\n","    'colsample_bytree': [0.6, 0.8, 1.0],\n","    'gamma': [0, 0.1, 0.2],\n","    'reg_alpha': [0, 0.1, 0.5],\n","    'reg_lambda': [0, 0.1, 0.5]\n","}\n","\n","# Initialize lists to store results\n","results = []\n","feature_importance_results = []\n","\n","# Initialize XGBoost regressor\n","xgb_reg = XGBRegressor(random_state=42)\n","\n","# Iterate over hyperparameter tests\n","for i, params in enumerate(ParameterGrid(param_grid)):\n","    print(f\"Testing Hyperparameter Set {i + 1}/{len(ParameterGrid(param_grid))}\")\n","\n","    # Train-test split\n","    X_train, X_test, y_train, y_test = train_test_split(model_df[feature_set], model_df['Views'], test_size=0.2, random_state=42)\n","\n","    # Initialize XGBoost regressor with current hyperparameters\n","    xgb_reg.set_params(**params)\n","\n","    # Train the model\n","    xgb_reg.fit(X_train, y_train)\n","\n","    # Make predictions (continuous)\n","    y_pred_continuous = xgb_reg.predict(X_test)\n","\n","    # Convert to binary predictions using a threshold (e.g., 0.5)\n","    y_pred_binary = (y_pred_continuous > 0.5).astype(int)\n","\n","    # Calculate evaluation metrics for regression\n","    mae = mean_absolute_error(y_test, y_pred_continuous)\n","    mse = mean_squared_error(y_test, y_pred_continuous)\n","    r2 = r2_score(y_test, y_pred_continuous)\n","\n","    # Calculate evaluation metrics for classification\n","    accuracy = accuracy_score(y_test, y_pred_binary)\n","    precision = precision_score(y_test, y_pred_binary)\n","    recall = recall_score(y_test, y_pred_binary)\n","    f1 = f1_score(y_test, y_pred_binary)\n","    roc_auc = roc_auc_score(y_test, y_pred_continuous)\n","\n","    # Store results for both regression and classification metrics\n","    results.append({\n","        'Hyperparameters': params,\n","        'MAE': mae,\n","        'MSE': mse,\n","        'R2': r2,\n","        'Accuracy': accuracy,\n","        'Precision': precision,\n","        'Recall': recall,\n","        'F1 Score': f1,\n","        'ROC AUC': roc_auc\n","    })\n","\n","    print(f\"  MAE: {mae}, MSE: {mse}, R2: {r2}\")\n","    print(f\"  Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}, ROC AUC: {roc_auc}\")\n","\n","# Convert results to DataFrame\n","results_df = pd.DataFrame(results)\n","\n","results_df.to_csv(\"/content/drive/My Drive/Youtube_Folder/xgboost_results_for_parameters.csv\", index=False)\n","\n","# Display results\n","print(results_df)\n","\n","# Here is the model I chose based on number of features and based on the metrics I used to measure performance\n","\n","# Selected features\n","feature_set = ['Like Count', 'Likes to Views Ratio', 'Avg Views Last 3 Months', 'duration_seconds',\n","               'Video Licensed Code', 'Game Played Corrected Code', 'Primary Social Media Link',\n","               'Premier Stream Video Code', 'Time of Day Code', 'Top Selling Game 2024']\n","\n","# Selected hyperparameters\n","params = {\n","    'colsample_bytree': 1.0, #Sampling each and every feature for all samples\n","    'gamma': 0, #Essentially no regularization being used\n","    'learning_rate': 0.2, #Because it is a lower value, it makes the model more robust by shrinking the weights on each step\n","    'max_depth': 5, #5 decisions are made before it comes to a conclusion on if the video will be higher/lower than 100k views\n","    'min_child_weight': 2, #Each child must have 2 instances. This will hopefully prevent overfitting\n","    'reg_alpha': 0, # no L1 regularization is applied to the weights of the model.\n","    'reg_lambda': 0.5,#a moderate level of L2 regularization. It strikes a balance between preventing overfitting and allowing the model to learn from the data effectively.\n","    'subsample': 1.0 #All samples are being used to validate the model\n","}\n","\n","# Initialize XGBoost regressor with specified hyperparameters\n","xgb_reg = XGBRegressor(**params, random_state=42)\n","\n","# Train the model\n","xgb_reg.fit(model_df[feature_set], model_df['Views'])\n","\n","# Print the model\n","print(xgb_reg)\n","\n","\n","\n"]}]}