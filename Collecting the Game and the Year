{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOsVzhEJqO2desdldOhf9gG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ghiLY0PErAY","executionInfo":{"status":"ok","timestamp":1709583452835,"user_tz":480,"elapsed":39854,"user":{"displayName":"Patrick Sullivan","userId":"07409751838678152568"}},"outputId":"ff1ba70d-8844-415e-87e3-e4d80c2c22b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Total videos remaining to be processed: 0\n","All videos have been added to the CSV file.\n","Total processing time: 17.361059427261353 seconds.\n"]}],"source":["import pandas as pd\n","import requests\n","import time\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# File paths\n","search_results_file_path = '/content/drive/My Drive/Capstone Project 2/search_results_video_info_no_shorts.csv'\n","video_info_file_path = '/content/drive/My Drive/Capstone Project 2/video_activity_channel.csv'\n","\n","# Function to fetch data from URL synchronously with retries\n","def fetch_data_with_retries(url, retries=3, timeout=60):\n","    for _ in range(retries):\n","        try:\n","            print(f\"Fetching data from URL: {url}\")  # Print the URL being accessed\n","            response = requests.get(url, timeout=timeout)\n","            if response.status_code == 200:\n","                return response.json()\n","            elif response.status_code == 404:\n","                print(\"URL not found.\")\n","                return None\n","            elif response.status_code == 429:\n","                print(\"Rate limit exceeded. Waiting before retrying...\")\n","                time.sleep(60)  # Wait for a minute before retrying\n","            else:\n","                print(f\"Unexpected status code: {response.status_code}. Retrying...\")\n","                time.sleep(30)  # Adding a small delay before retrying\n","        except requests.Timeout:\n","            print(\"Timeout occurred. Retrying...\")\n","            time.sleep(30)  # Adding a small delay before retrying\n","        except Exception as e:\n","            print(f\"Error occurred: {e}. Retrying...\")\n","            time.sleep(30)  # Adding a small delay before retrying\n","    print(\"Failed after multiple retries. Skipping this batch.\")\n","    return None\n","\n","# Function to process all batches and append to CSV\n","def main():\n","    # Load CSV file containing video IDs\n","    search_results_df = pd.read_csv(search_results_file_path)\n","    # Extract video IDs\n","    video_ids_all = set(search_results_df['Video ID'].unique())\n","    # If there are no new video IDs, exit\n","    if not video_ids_all:\n","        print(\"No new video IDs to fetch.\")\n","        return\n","\n","    # Check the CSV file to see which videos still need to be processed\n","    processed_video_ids = set()\n","    try:\n","        processed_video_df = pd.read_csv(video_info_file_path)\n","        processed_video_ids = set(processed_video_df['Video ID'])\n","    except FileNotFoundError:\n","        pass\n","\n","    video_ids_remaining = list(video_ids_all - processed_video_ids)\n","    total_videos_remaining = len(video_ids_remaining)\n","    print(f\"Total videos remaining to be processed: {total_videos_remaining}\")\n","\n","    # Reverse the order of video IDs\n","    video_ids_remaining = video_ids_remaining[::-1]\n","\n","    # Asynchronously fetch data for remaining videos\n","    batch_rows = []\n","    for i in range(0, total_videos_remaining, 50):\n","        video_ids_batch = video_ids_remaining[i:i + 50]\n","        video_id_string_batch = ','.join(video_ids_batch)\n","        url = f\"https://yt4.lemnoslife.com/videos?part=activity&id={video_id_string_batch}\"\n","        data = fetch_data_with_retries(url)\n","\n","        if data is not None and 'items' in data:\n","            for item in data['items']:\n","                # Process each item and append to batch_rows\n","                video_id = item['id']\n","                activity = item.get('activity', {})\n","\n","                # Extract Published At, Description, and Title\n","                game_listed = activity.get('name', '')\n","                game_year = activity.get('year', '')\n","\n","                # Append item to batch_rows if video ID is not already in CSV\n","                if video_id not in processed_video_ids:\n","                    batch_rows.append({\n","                        'Video ID': video_id,\n","                        'Game Played': game_listed,#I found that this held what game was played. The creator sometimes had it blank though\n","                        'Game Year': game_year #Interested to know if year of the game mattered\n","                    })\n","                    processed_video_ids.add(video_id)\n","\n","        # Append to CSV after processing each batch of 50 videos\n","        if len(batch_rows) >= 50 or i + 50 >= total_videos_remaining:\n","            append_to_csv(batch_rows)\n","            batch_rows = []\n","\n","            # Print remaining videos\n","            remaining_videos = total_videos_remaining - (i + len(batch_rows))\n","            print(f\"Remaining videos to be processed: {remaining_videos}\")\n","\n","    print(\"All videos have been added to the CSV file.\")\n","\n","# Function to append to CSV\n","def append_to_csv(batch_rows):\n","    if batch_rows:\n","        df = pd.DataFrame(batch_rows)\n","        mode = 'a' if pd.read_csv(video_info_file_path, nrows=1).empty else 'w'\n","        df.to_csv(video_info_file_path, mode=mode, index=False, header=mode=='w')\n","        print(\"Videos have been added to the CSV file.\")\n","\n","# Run the main function\n","start_time = time.time()\n","main()\n","end_time = time.time()\n","\n","# Total processing time\n","print(f\"Total processing time: {end_time - start_time} seconds.\")\n"]}]}